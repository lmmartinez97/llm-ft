{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to load the config.json where we have the TOKEN from Huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = json.load(open('config.json'))\n",
    "HF_TOKEN = config_data['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicate the model you want to use\n",
    "In this case, we are using the Llama3 model from Meta, we can found it in the Hugging Face model hub [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to configure the quantization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If need quantization, set the following config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tokenizer and model\n",
    "\n",
    "In the context of language models like Llama3, `tokenizer` is an instance of a tokenizer class that is used to convert text into a format that the model can understand.\n",
    "\n",
    "`tokenizer.pad_token` and `tokenizer.eos_token` are special tokens used in the tokenization process:\n",
    "\n",
    "- `pad_token`: This is the token used for padding. Padding is necessary because when processing multiple sequences in a batch, all sequences need to be the same length. If a sequence is shorter than others, it's filled (or \"padded\") with this token to match the length of the longest sequence.\n",
    "\n",
    "- `eos_token`: This is the \"end of sequence\" token. It's used to signal the end of a sequence. This is particularly important in tasks like language generation where the model needs to know when to stop generating text.\n",
    "\n",
    "The line `tokenizer.pad_token = tokenizer.eos_token` is setting the padding token to be the same as the end of sequence token. This might be done for a variety of reasons, depending on the specifics of the task and the model. For example, in some tasks, it might be beneficial to treat padded areas of the sequence as though they are the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer using quantization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we create the model pre-trained (this takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:45<00:00, 41.39s/it]\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', token=HF_TOKEN)\n",
    "# If you want to use quantization, use the following line instead\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', quantization_config=bnb_config, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now need to save the model and tokenizer to the desired location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('LLAMA-3-8B-Instruct-Quant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LLAMA-3-8B-Instruct-tokenizer-Quant/tokenizer_config.json',\n",
       " 'LLAMA-3-8B-Instruct-tokenizer-Quant/special_tokens_map.json',\n",
       " 'LLAMA-3-8B-Instruct-tokenizer-Quant/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('LLAMA-3-8B-Instruct-tokenizer-Quant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "Now we have model files in the 'LLAMA-3-8B-Instruct' and his tokenizer directory at local. Load them to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = json.load(open('config.json'))\n",
    "HF_TOKEN = config_data['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'LLAMA-3-8B-Instruct-Quant'\n",
    "tokenizer_id = 'LLAMA-3-8B-Instruct-tokenizer-Quant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer that we just saved\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline for text generation\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "sys_content = \"You are a an assitant\"\n",
    "user_content = None\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_content},\n",
    "    {\"role\": \"user\", \"content\": user_content},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you can use the pipeline to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a an assitant'}, {'role': 'user', 'content': 'What is the weather like today?'}]\n",
      "According to the latest reports, it's a beautiful day out there! The weather is mostly sunny with a high of 72°F (22°C) and a gentle breeze of 5mph (8km/h). There's a slight chance of scattered clouds later in the afternoon, but overall, it's looking like a perfect day to get outside and enjoy the sunshine!\n"
     ]
    }
   ],
   "source": [
    "user_content = \"What is the weather like today?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_content},\n",
    "    {\"role\": \"user\", \"content\": user_content},\n",
    "]\n",
    "\n",
    "print(messages)\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(user_content):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can check that for you! According to the latest forecast, it's a beautiful day today with plenty of sunshine and a high of 75°F (24°C). There's a gentle breeze blowing at about 10 mph (16 km/h), making it a perfect day to get outside and enjoy the fresh air. However, please note that weather conditions can change rapidly, so it's always a good idea to check the forecast again before heading out. Would you like me to check the weather for a specific location or provide more details about the forecast?\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"What is the weather like today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To create a new user in Linux, you can use the `useradd` command. Here\\'s the basic syntax:\\n\\n`useradd [options] username`\\n\\nHere, `username` is the name you want to give to the new user, and `options` are optional arguments that you can use to customize the user creation process.\\n\\nHere are some common options you can use with `useradd`:\\n\\n* `-m` or `--create-home`: Create a home directory for the new user.\\n* `-s` or `--shell`: Specify the default shell for the new user.\\n* `-p` or `--password`: Set the password for the new user.\\n* `-g` or `--group`: Specify the group that the new user should belong to.\\n\\nFor example, to create a new user named \"john\" with a home directory and a default shell, you can use the following command:\\n\\n`useradd -m john`\\n\\nThis will create a new user named \"john\" with a home directory and a default shell. You can then set the password for the new user using the `passwd` command:\\n\\n`passwd john`\\n\\nYou can also use the `useradd` command with options to customize the user creation process. For example, to create a new'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"How creat a new user in linux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_content = \\\n",
    "\"\"\"\n",
    "You are an API, you will receive a message and must respond with an array with each word in the message.\n",
    "You must write only the array of words or numbers if they are separated by spaces\n",
    "\"\"\"\n",
    "user_content = \"Hello, how are you 333?\"\n",
    "\n",
    "array = generate_text(user_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello\", \"how\", \"are\", \"you\", \"333\"] this is the array generated by the model\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(array, \"this is the array generated by the model\")\n",
    "print(type(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'how', 'are', 'you', '333']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "s = array\n",
    "array = json.loads(s)\n",
    "\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(array[-1])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "how\n",
      "are\n",
      "you\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "for word in array:\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
